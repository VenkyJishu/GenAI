{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "txt is If # everything Hello@ everyone . Bye 99 is 34working correctly, < this testing in hospital for patient > bye> EveryOne HhEllo\n",
      "after clean If   everything Hello  everyone  Bye    is   working correctly   this testing in hospital for patient   bye  EveryOne HhEllo\n",
      "token is If\n",
      "token is   \n",
      "token is everything\n",
      "token is Hello\n",
      "token.text is Hello\n",
      "token is  \n",
      "token is everyone\n",
      "token is  \n",
      "token is Bye\n",
      "token is    \n",
      "token is is\n",
      "token is   \n",
      "token is working\n",
      "token is correctly\n",
      "token is   \n",
      "token is this\n",
      "token is testing\n",
      "token is in\n",
      "token is hospital\n",
      "token is for\n",
      "token is patient\n",
      "token is   \n",
      "token is bye\n",
      "token is  \n",
      "token is EveryOne\n",
      "token is HhEllo\n",
      "Processed texts: [['Hello']]\n",
      "Time taken: 0.00 seconds\n",
      "{'should', 'please', 'me', 'serious', 'otherwise', 'over', 'anyone', 'also', '’ve', 'whereupon', 'becomes', 'full', 'five', 'him', '’m', '’d', 'is', '’ll', 'everywhere', '‘m', 'whenever', 'seemed', 'else', 'amount', 'up', 'one', 'anyway', 'more', 'through', 'thereby', 'everything', 'off', 'say', 'other', 'between', 'first', 'together', 'himself', 'hers', 'or', '‘ll', 'seem', 'we', 'as', 'nor', 'afterwards', 'however', 'there', 'thru', 'against', 'somehow', 'regarding', 'us', 'across', 'always', 'thereafter', 'this', 'herein', 'part', 'here', 'few', 'much', 'towards', 'yet', 'was', 'many', 'last', 'until', 'then', 'the', \"'re\", 'used', 'both', 'doing', 'whither', 'though', 'upon', 'n‘t', 'every', 'seems', 'fifteen', 'anywhere', 'among', 'besides', 'around', 'another', 'whence', 'hereafter', 'wherein', 'his', 'am', 'beside', 'none', 'yourselves', \"n't\", 'he', 'whose', 'might', 'again', 'she', 'its', 'own', 'meanwhile', 'during', 'are', 'some', 'empty', 'that', 'too', 'somewhere', 'several', 'unless', 'via', 'each', 'nowhere', 'anyhow', 'everyone', 'already', 'myself', 'former', 'nevertheless', 'various', '‘s', 'had', 'either', 'now', 'of', 'hereupon', 'about', 'not', 'whether', 'neither', 'yourself', 'whereas', 'four', 'eleven', 'how', 'almost', 'well', 'n’t', 'any', 'ourselves', 'still', 'three', 'before', 'such', 'elsewhere', 'but', 'noone', 'sixty', 'due', \"'m\", 'would', 'formerly', 'least', 'must', 'six', \"'d\", 'whom', 'may', \"'ll\", 'two', 'ours', 'because', '‘re', 'nothing', 'bottom', 'perhaps', 'my', 'no', 'put', 'eight', 'namely', 'on', 'take', 'become', 'third', 'top', 'after', 'were', 'while', 'to', 'does', 'within', 'enough', 'alone', 'along', 'without', 'toward', 'get', 'into', 'show', 'have', 'with', 'under', 'others', 'go', 'keep', 'something', 'above', 'sometimes', 'when', 'less', 'them', 'has', 'so', 'back', 'at', 'move', 'fifty', 'a', 'give', 'same', 'front', 'than', 'make', 'they', 'side', 'wherever', 'hereby', 'out', 'therefore', 'cannot', 'ca', 'down', 'whoever', 'herself', 'could', 'did', 'amongst', 'thereupon', 'being', 'latter', 'do', 'per', 'whatever', 'nine', 'even', 're', 'nobody', 'made', 'mostly', 'ever', 'and', 'therein', 'onto', 'in', 'further', 'beforehand', 'beyond', 'those', 'indeed', '’s', 'an', 'never', 'ten', 'seeming', 'twenty', '‘ve', 'by', 'whereby', 'mine', 'since', 'their', 'call', 'becoming', 'forty', 'although', 'be', 'you', 'been', 'very', 'sometime', 'your', 'can', 'why', 'for', 'really', 'itself', 'see', 'throughout', 'anything', 'name', 'where', 'whole', 'rather', 'it', 'whereafter', 'twelve', 'themselves', 'which', 'if', 'except', 'next', 'only', 'done', \"'ve\", 'became', 'hundred', 'yours', 'once', 'just', 'what', 'often', 'latterly', '‘d', 'most', 'below', 'i', 'will', '’re', 'from', 'our', 'thus', 'behind', 'all', 'hence', 'using', 'thence', \"'s\", 'someone', 'who', 'her', 'moreover', 'these', 'quite'}\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import spacy\n",
    "import re\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "\n",
    "# Load SpaCy model (use 'en_core_web_sm' for English)\n",
    "# Only use tokenizer, skip parser and NER for performance.\n",
    "nlp = spacy.load('en_core_web_sm')  # Disable unnecessary components\n",
    "\n",
    "# Preprocess text: Clean up using regex to remove unwanted tokens.\n",
    "def clean_text(text):\n",
    "    # Remove hashtags, mentions, digits, special characters except for spaces\n",
    "    text = re.sub(r'[@#\\d<>]', ' ', text)  # Remove @, #, digits, <>, etc.\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)    # Remove all punctuation except space\n",
    "    print('after clean',text)\n",
    "    return text\n",
    "\n",
    "def pre_process_batch(texts):\n",
    "    # Initialize set to track unique lemmatized words (case-insensitive)\n",
    "    existed_tokens = set()\n",
    "    filtered_tokens_list = []\n",
    "\n",
    "    # Process each text (sentence/document)\n",
    "    for txt in texts:\n",
    "        print(f'txt is {txt}')\n",
    "        # Clean the text before tokenization\n",
    "        cleaned_txt = clean_text(txt)\n",
    "        \n",
    "        # Tokenize the cleaned text using SpaCy's tokenizer directly (skip everything else)\n",
    "        doc = nlp.make_doc(cleaned_txt)\n",
    "        \n",
    "        filtered_tokens = []\n",
    "        for token in doc:\n",
    "            print(f\"token is {token}\")\n",
    "            lemmatized_word = token.lemma_.lower()\n",
    "\n",
    "            # Filter out stopwords, digits, punctuation, and ensure uniqueness\n",
    "            if lemmatized_word not in existed_tokens and \\\n",
    "               token.text.lower() not in STOP_WORDS and \\\n",
    "               token.text.isalpha():  # No need to check isdigit() because we pre-cleaned digits\n",
    "                print(f\"token.text is {token.text}\")\n",
    "                filtered_tokens.append(token.text)\n",
    "                existed_tokens.add(lemmatized_word)  # Track lemmatized word to ensure uniqueness\n",
    "        \n",
    "        filtered_tokens_list.append(filtered_tokens)\n",
    "\n",
    "    return filtered_tokens_list\n",
    "\n",
    "# Example: List of 4000 text entries\n",
    "texts = [\n",
    "    \"If # everything Hello@ everyone . Bye 99 is 34working correctly, < this testing in hospital for patient > bye> EveryOne HhEllo\"\n",
    "   # \"Add more sentences here...\"\n",
    "]\n",
    "\n",
    "# Process the batch of texts\n",
    "import time\n",
    "start_time = time.time()\n",
    "processed_texts = pre_process_batch(texts)\n",
    "print(\"Processed texts:\", processed_texts)  # Print only the first 5 results for inspection\n",
    "print(f\"Time taken: {time.time() - start_time:.2f} seconds\")\n",
    "print(STOP_WORDS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed texts: [['Hello']]\n",
      "Time taken: 0.00 seconds\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import spacy\n",
    "import re\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "\n",
    "# Load SpaCy model (use 'en_core_web_sm' for English)\n",
    "# Only use tokenizer, skip parser and NER for performance.\n",
    "nlp = spacy.load('en_core_web_sm', disable=['ner', 'parser', 'tagger'])  # Disable unnecessary components\n",
    "\n",
    "# Preprocess text: Clean up using regex to remove unwanted tokens.\n",
    "def clean_text(text):\n",
    "    # Remove unwanted symbols: @, #, <>, and digits (keeping the spaces between words)\n",
    "    text = re.sub(r'[@#<>]', ' ', text)  # Remove @, #, <, >\n",
    "    text = re.sub(r'\\d+', ' ', text)     # Remove digits (keep spaces between words)\n",
    "    return text\n",
    "\n",
    "def pre_process_batch(texts):\n",
    "    # Initialize set to track unique lemmatized words (case-insensitive)\n",
    "    existed_tokens = set()\n",
    "    filtered_tokens_list = []\n",
    "\n",
    "    # Process each text (sentence/document)\n",
    "    for txt in texts:\n",
    "        # Clean the text before tokenization\n",
    "        cleaned_txt = clean_text(txt)\n",
    "        \n",
    "        # Tokenize the cleaned text using SpaCy's tokenizer directly (skip everything else)\n",
    "        doc = nlp.make_doc(cleaned_txt)\n",
    "        \n",
    "        filtered_tokens = []\n",
    "        for token in doc:\n",
    "            lemmatized_word = token.lemma_.lower()\n",
    "\n",
    "            # Filter out stopwords, ensure uniqueness, and retain only valid tokens\n",
    "            if lemmatized_word not in existed_tokens and \\\n",
    "               token.text.lower() not in STOP_WORDS and \\\n",
    "               token.text.isalpha():  # Only retain alphabetic tokens\n",
    "                filtered_tokens.append(token.text)\n",
    "                existed_tokens.add(lemmatized_word)  # Track lemmatized word to ensure uniqueness\n",
    "        \n",
    "        filtered_tokens_list.append(filtered_tokens)\n",
    "\n",
    "    return filtered_tokens_list\n",
    "\n",
    "# Example: List of 4000 text entries\n",
    "texts = [\n",
    "    \"If # everything Hello@ everyone . Bye 99 is 34working correctly, < this testing in hospital for patient > bye> EveryOne\",\n",
    "    # Add more sentences here...\n",
    "]\n",
    "\n",
    "# Process the batch of texts\n",
    "import time\n",
    "start_time = time.time()\n",
    "processed_texts = pre_process_batch(texts)\n",
    "print(\"Processed texts:\", processed_texts[:5])  # Print only the first 5 results for inspection\n",
    "print(f\"Time taken: {time.time() - start_time:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
